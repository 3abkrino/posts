---
message: |-
  Anyone experienced in webscraping here? I am creating an excel sheet that collects data from different websites regarding the prices and it also autoupdate the price based on any change.

  I can do this on googlesheets using XML and XPath.
   But I rather want it on excel
from:
  name: Mina Joseph
  id: '10160048984645694'
type: status
created_time: '2017-07-02T00:02:31+0000'
updated_time: '2017-07-02T17:46:19+0000'
permalink_url: https://www.facebook.com/groups/egyptian.geeks/permalink/1533999246639778/
id: '172338516139198_1533999246639778'
comments:
  data:
  - created_time: '2017-07-02T00:10:12+0000'
    from:
      name: Mohamed Ashraf
      id: '10155906116946287'
    message: |-
      I cant help you much but I know excel has a vb scripting language baked inside that you may be able to use for stuff like that.

      It just seems weird why you are limiting yourself to a weird programming language and features when you can just do the webscrapping part separately in a better suited language (like python with BeautifulSoup)
    id: '1534006386639064'
  - created_time: '2017-07-02T00:12:19+0000'
    from:
      name: Mina Joseph
      id: '10160048984645694'
    message: I am still studying Python. Though question is can I pull the data and
      have it refreshing on excel or any user (non geek) friendly medium where they
      can see live price update?
    id: '1534007713305598'
  - created_time: '2017-07-02T04:57:25+0000'
    from:
      name: Mohamed Ehab
      id: '790078684517700'
    message: |-
      I believe you can do so using VBA.
      What I understand is that you let the scrapper run periodically.
      If yes, you can do so in VBA as far as I remember (I guess it has something like xmlHttpRequest)
    id: '1534251823281187'
  - created_time: '2017-07-02T17:34:47+0000'
    from:
      name: Mohamed Salah
      id: '10159893974840162'
    message: |-
      Hello Mina, developing a web scrapper looks like an easy task but the more sophisticated the sites area, the more would a real programming language be required. There are many tools online that do what you want, Octoparse is one good tool, Kimono Labs was great before itgot closed, they don't require programming skills and output a csv file. Google sheets has a limit on the external XML requests to only a few hundred calls per account per day.

      However for more sophisticated sites you need a sophisticated script. You should ask yourself a few things:

      - is the site checking on user cookies before accepting a request?
      - is the site using a technology lime cloud flare that would block your crawler after a while?

      - Is the site too large for easy tools like these to use (10,000+ pages)

      If the answer is yes to any of these questions, then it would be wise to invest time in writing a script in python or php.
    id: '1534803069892729'
  - created_time: '2017-07-02T17:46:16+0000'
    from:
      name: Mina Joseph
      id: '10160048984645694'
    message: Thanks guys.
    id: '1534813516558351'
  paging:
    cursors:
      before: WTI5dGJXVnVkRjlqZAFhKemIzSTZANVFV6TkRBd05qTTROall6T1RBMk5Eb3hORGs0T1RVME1qRXoZD
      after: WTI5dGJXVnVkRjlqZAFhKemIzSTZANVFV6TkRneE16VXhOalUxT0RNMU1Ub3hORGs1TURFM05UYzMZD
